# üîç Venice API Insights from Swagger Documentation

## Key Discoveries

### ‚úÖ What I Learned

#### 1. **Default Model**
- **llama-3.3-70b** is the default model for Venice API
- It supports `supportsResponseSchema: true`
- Perfect for JSON structured outputs

#### 2. **Response Format Support**
Not all models support `response_format` with `json_schema`. Only models with:
- `supportsFunctionCalling: true`
- `supportsResponseSchema: true`

**Models that support JSON Schema:**
- ‚úÖ llama-3.3-70b (default)
- ‚úÖ llama-3.2-3b (fastest)
- ‚úÖ Models with function calling support

#### 3. **Proper Response Format Structure**
```typescript
response_format: {
  type: 'json_schema',
  json_schema: {
    properties: {
      // Your schema here
    },
    required: ['field1', 'field2']
  }
}
```

#### 4. **Model Capabilities**
Each model has specific capabilities:
```yaml
capabilities:
  optimizedForCode: boolean
  quantization: string (fp16, fp8)
  supportsFunctionCalling: boolean
  supportsReasoning: boolean
  supportsResponseSchema: boolean  # ‚Üê Important!
  supportsVision: boolean
  supportsWebSearch: boolean
  supportsLogProbs: boolean
```

---

## üîß What I Fixed

### Changed Model from qwen3-235b to llama-3.3-70b

**Before:**
```typescript
model: 'qwen3-235b' // This model wasn't in the Swagger docs
```

**After:**
```typescript
model: 'llama-3.3-70b' // Default model, supports response_format
```

**Why:**
- qwen3-235b doesn't appear in the Swagger documentation
- llama-3.3-70b is the default model
- Confirmed to support JSON schema responses
- More reliable and tested

### Improved System Prompt

**Before:**
```typescript
content: 'You are an expert AI educator...'
```

**After:**
```typescript
content: 'You are an expert AI educator who creates personalized learning curricula. You MUST respond with ONLY valid JSON matching the exact schema provided. Do not include any text outside the JSON object.'
```

**Why:**
- Explicitly instructs to return JSON only
- Prevents text before/after JSON
- Reduces parsing errors

---

## üìä Model Comparison

| Model | Context | Response Schema | Function Calling | Speed | Best For |
|-------|---------|----------------|------------------|-------|----------|
| llama-3.3-70b | 65K | ‚úÖ Yes | ‚úÖ Yes | Medium | **Course Generation** ‚ú® |
| llama-3.2-3b | 131K | ‚úÖ Yes | ‚úÖ Yes | Fast | Quick responses |
| llama-3.1-405b | 65K | ‚ùå No | ‚ùå No | Slow | Complex reasoning |
| venice-uncensored | 32K | ? | ? | Medium | Uncensored content |

---

## üéØ Recommended Setup

### For Course Generation
```typescript
{
  model: 'llama-3.3-70b',  // Default, reliable
  temperature: 0.7,         // Balanced creativity
  max_tokens: 16000,        // Large enough for 10 chapters
  response_format: {        // Enforces JSON structure
    type: 'json_schema',
    json_schema: { /* schema */ }
  }
}
```

### For Chapter Simplification
```typescript
{
  model: 'llama-3.3-70b',  // Same model for consistency
  temperature: 0.7,
  max_tokens: 4000,         // Smaller for single chapter
  // No response_format needed (returns markdown)
}
```

---

## üêõ Common Issues & Solutions

### Issue 1: "Model doesn't support response_format"
**Solution:** Use llama-3.3-70b or llama-3.2-3b (both support it)

### Issue 2: "Invalid JSON response"
**Solution:**
- Check model supports `supportsResponseSchema: true`
- Add explicit "JSON only" instruction in system prompt
- Try with simpler schema first

### Issue 3: "Max tokens exceeded"
**Solution:**
- llama-3.3-70b has 65K context
- Our request uses ~16K tokens max
- Should be fine for 10 chapters

### Issue 4: "API key invalid"
**Solution:**
- Keys are now hard-coded in route.ts files
- Check: `const VENICE_API_KEY = 'ntmhtbP2fr...'`

---

## üìù API Request Example

### Working Course Generation Request
```typescript
const response = await fetch('https://api.venice.ai/api/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Authorization': 'Bearer ntmhtbP2fr_pOQsmuLPuN_nm6lm2INWKiNcvrdEfEC',
    'Content-Type': 'application/json'
  },
  body: JSON.stringify({
    model: 'llama-3.3-70b',
    messages: [
      {
        role: 'system',
        content: 'You MUST respond with ONLY valid JSON...'
      },
      {
        role: 'user',
        content: 'Create a course about...'
      }
    ],
    temperature: 0.7,
    max_tokens: 16000,
    response_format: {
      type: 'json_schema',
      json_schema: {
        type: 'object',
        properties: {
          title: { type: 'string' },
          chapters: {
            type: 'array',
            items: {
              type: 'object',
              properties: {
                chapterNumber: { type: 'number' },
                title: { type: 'string' },
                content: { type: 'string' }
              },
              required: ['chapterNumber', 'title', 'content']
            }
          }
        },
        required: ['title', 'chapters']
      }
    }
  })
})
```

### Expected Response
```json
{
  "id": "chatcmpl-xxx",
  "object": "chat.completion",
  "created": 1699000000,
  "model": "llama-3.3-70b",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "{\"title\":\"Your Course\",\"chapters\":[...]}"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 500,
    "completion_tokens": 8000,
    "total_tokens": 8500
  }
}
```

---

## üöÄ Testing Checklist

After these changes, test:

1. **‚úÖ Persona Selection**
   - Click "Quick Start"
   - Select any persona
   - Watch console for API call

2. **‚úÖ Check Console Logs**
   ```
   Received user profile: {...}
   Calling Venice API with llama-3.3-70b model...
   Venice API response received
   ```

3. **‚úÖ Verify Course Generation**
   - Should complete in 30-45 seconds
   - Should return valid JSON
   - Should parse without errors

4. **‚úÖ Check Browser Network Tab**
   - Request to `/api/generate-course`
   - Status: 200 (not 500!)
   - Response has valid course data

---

## üí° Why This Works Better

### Before (qwen3-235b):
- ‚ùå Model not in Swagger docs
- ‚ùå Uncertain support for response_format
- ‚ùå Could be causing 500 errors

### After (llama-3.3-70b):
- ‚úÖ Default Venice model
- ‚úÖ Confirmed response_format support
- ‚úÖ Well-documented and tested
- ‚úÖ 65K context (plenty for our use)
- ‚úÖ Balanced speed/quality

---

## üîÆ Next Steps

1. **Test with current setup** (llama-3.3-70b)
2. If successful, keep it
3. If issues persist:
   - Try llama-3.2-3b (faster)
   - Simplify JSON schema
   - Check browser console errors
   - Look at server terminal output

---

## üìö Additional Resources

- **Swagger File:** `swagger (4).yaml`
- **Venice Docs:** https://docs.venice.ai
- **API Endpoint:** https://api.venice.ai/api/v1

---

## ‚úÖ Summary

**What Changed:**
- ‚úÖ Using llama-3.3-70b (default, reliable model)
- ‚úÖ Improved system prompt for JSON responses
- ‚úÖ Hard-coded API keys for debugging
- ‚úÖ Verified model supports response_format

**Expected Result:**
- üéØ Course generation should work now
- üöÄ No more 500 errors
- ‚ú® Proper JSON responses
- üìä Clean, structured course data

**Test it:** Open http://localhost:3000 ‚Üí Quick Start ‚Üí Select persona ‚Üí Generate course

The app should now work correctly! üéâ

